apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-low-cost
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llama-low-cost
  template:
    metadata:
      labels:
        app: llama-low-cost
    spec:
      runtimeClassName: wasmedge
      containers:
        - name: llama-api-server
          image: ghcr.io/second-state/llama-api-server:latest
          imagePullPolicy: Never
          command: ["llama-api-server.wasm"]
          args:
            - "--prompt-template"
            - "llama-3-chat"
            - "--ctx-size"
            - "4096"
            - "--model-name"
            - "llama-3-1b-low"
          env:
            - name: WASMEDGE_PLUGIN_PATH
              value: "/home/dev/.wasmedge/plugin"

            - name: LD_LIBRARY_PATH
              value: "/home/dev/.wasmedge/lib"

            - name: WASMEDGE_WASINN_PRELOAD
              value: "default:GGML:CPU:/home/dev/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf"

          volumeMounts:
            - name: gguf-model-file
              mountPath: /home/dev/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
              readOnly: true

            - name: wasi-nn-plugin-file
              mountPath: /home/dev/.wasmedge/plugin/libwasmedgePluginWasiNN.so
              readOnly: true
            - name: wasi-nn-plugin-lib
              mountPath: /home/dev/.wasmedge/lib
              readOnly: true

            - name: libm
              mountPath: /lib/aarch64-linux-gnu/libm.so.6
              readOnly: true
            - name: libpthread
              mountPath: /lib/aarch64-linux-gnu/libpthread.so.0
              readOnly: true
            - name: libc
              mountPath: /lib/aarch64-linux-gnu/libc.so.6
              readOnly: true
            - name: ld-linux
              mountPath: /lib/ld-linux-aarch64.so.1
              readOnly: true
            - name: libdl
              mountPath: /lib/aarch64-linux-gnu/libdl.so.2
              readOnly: true
            - name: libstdcxx
              mountPath: /lib/aarch64-linux-gnu/libstdc++.so.6
              readOnly: true
            - name: libgcc-s
              mountPath: /lib/aarch64-linux-gnu/libgcc-s.so.1
              readOnly: true

      volumes:
        - name: gguf-model-file
          hostPath:
            path: /home/dev/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
            type: File

        - name: wasi-nn-plugin-file
          hostPath:
            path: /home/dev/.wasmedge/plugin/libwasmedgePluginWasiNN.so
            type: File
        - name: wasi-nn-plugin-lib
          hostPath:
            path: /home/dev/.wasmedge/lib
            type: Directory

        - name: libm
          hostPath:
            path: /lib/aarch64-linux-gnu/libm.so.6
            type: File
        - name: libpthread
          hostPath:
            path: /lib/aarch64-linux-gnu/libpthread.so.0
            type: File
        - name: libc
          hostPath:
            path: /lib/aarch64-linux-gnu/libc.so.6
            type: File
        - name: ld-linux
          hostPath:
            path: /lib/ld-linux-aarch64.so.1
            type: File
        - name: libdl
          hostPath:
            path: /lib/aarch64-linux-gnu/libdl.so.2
            type: File
        - name: libstdcxx
          hostPath:
            path: /lib/aarch64-linux-gnu/libstdc++.so.6
            type: File
        - name: libgcc-s
          hostPath:
            path: /lib/aarch64-linux-gnu/libgcc_s.so.1
            type: File

---
apiVersion: v1
kind: Service
metadata:
  name: llama-low-cost-service
spec:
  selector:
    app: llama-low-cost
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-high-cost
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llama-high-cost
  template:
    metadata:
      labels:
        app: llama-high-cost
    spec:
      runtimeClassName: wasmedge
      containers:
        - name: llama-api-server
          image: ghcr.io/second-state/llama-api-server:latest
          imagePullPolicy: Never
          command: ["llama-api-server.wasm"]
          args:
            - "--prompt-template"
            - "llama-3-chat"
            - "--ctx-size"
            - "4096"
            - "--model-name"
            - "llama-3-3b-high"
          env:
            - name: WASMEDGE_PLUGIN_PATH
              value: "/home/dev/.wasmedge/plugin"

            - name: LD_LIBRARY_PATH
              value: "/home/dev/.wasmedge/lib"

            - name: WASMEDGE_WASINN_PRELOAD
              value: "default:GGML:CPU:/home/dev/models/Llama-3.2-3B-Instruct-Q5_K_M.gguf"

          volumeMounts:
            - name: gguf-model-file
              mountPath: /home/dev/models/Llama-3.2-3B-Instruct-Q5_K_M.gguf
              readOnly: true

            - name: wasi-nn-plugin-file
              mountPath: /home/dev/.wasmedge/plugin/libwasmedgePluginWasiNN.so
              readOnly: true
            - name: wasi-nn-plugin-lib
              mountPath: /home/dev/.wasmedge/lib
              readOnly: true

            - name: libm
              mountPath: /lib/aarch64-linux-gnu/libm.so.6
              readOnly: true
            - name: libpthread
              mountPath: /lib/aarch64-linux-gnu/libpthread.so.0
              readOnly: true
            - name: libc
              mountPath: /lib/aarch64-linux-gnu/libc.so.6
              readOnly: true
            - name: ld-linux
              mountPath: /lib/ld-linux-aarch64.so.1
              readOnly: true
            - name: libdl
              mountPath: /lib/aarch64-linux-gnu/libdl.so.2
              readOnly: true
            - name: libstdcxx
              mountPath: /lib/aarch64-linux-gnu/libstdc++.so.6
              readOnly: true
            - name: libgcc-s
              mountPath: /lib/aarch64-linux-gnu/libgcc-s.so.1
              readOnly: true

      volumes:
        - name: gguf-model-file
          hostPath:
            path: /home/dev/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
            type: File

        - name: wasi-nn-plugin-file
          hostPath:
            path: /home/dev/.wasmedge/plugin/libwasmedgePluginWasiNN.so
            type: File
        - name: wasi-nn-plugin-lib
          hostPath:
            path: /home/dev/.wasmedge/lib
            type: Directory

        - name: libm
          hostPath:
            path: /lib/aarch64-linux-gnu/libm.so.6
            type: File
        - name: libpthread
          hostPath:
            path: /lib/aarch64-linux-gnu/libpthread.so.0
            type: File
        - name: libc
          hostPath:
            path: /lib/aarch64-linux-gnu/libc.so.6
            type: File
        - name: ld-linux
          hostPath:
            path: /lib/ld-linux-aarch64.so.1
            type: File
        - name: libdl
          hostPath:
            path: /lib/aarch64-linux-gnu/libdl.so.2
            type: File
        - name: libstdcxx
          hostPath:
            path: /lib/aarch64-linux-gnu/libstdc++.so.6
            type: File
        - name: libgcc-s
          hostPath:
            path: /lib/aarch64-linux-gnu/libgcc_s.so.1
            type: File

---
apiVersion: v1
kind: Service
metadata:
  name: llama-high-cost-service
spec:
  selector:
    app: llama-high-cost
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-balancer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: load-balancer
  template:
    metadata:
      labels:
        app: load-balancer
    spec:
      runtimeClassName: wasmedge
      containers:
        - name: load-balancer
          image: ghcr.io/second-state/load-balancer-llamaedge:latest
          imagePullPolicy: Never
          command: ["load_balancer.wasm"]
          env:
            - name: SERVICES
              value: "llama-low-cost-service,3;llama-high-cost-service,1"
            # Add the missing environment variables
            - name: LLAMA_LOW_COST_SERVICE_HOST
              value: "llama-low-cost-service"
            - name: LLAMA_LOW_COST_SERVICE_PORT
              value: "8080"
            - name: LLAMA_HIGH_COST_SERVICE_HOST
              value: "llama-high-cost-service"
            - name: LLAMA_HIGH_COST_SERVICE_PORT
              value: "8080"
          ports:
            - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: load-balancer-service
spec:
  selector:
    app: load-balancer
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP

---
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: wasmedge
handler: wasmedge
